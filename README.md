# Deep Control
## Simple Pytorch implementations of Deep RL algorithms for continuous control research

This repository contains re-implementations of Deep RL algorithms for continuos action spaces. Some highlights:

1) Code is readable, and written to be easy to modify for future research.
2) Train and Test on different environments (for generalization research).
3) Built-in Tensorboard logging, parameter saving.
4) Support for offline (batch) RL.
5) Quick setup for benchmarks like Gym Mujoco, Atari, Pybullet, and DeepMind Control Suite.
5) Separate training and learning routines, which make it easy to mix and match techniques that improve the training process with techniques that improve the learning update.

### What's included?

#### Deep Deterministic Policy Gradient (DDPG)
Paper: [Continuous control with deep reinforcement learning](https://arxiv.org/abs/1509.02971), Lillicrap et al., 2015.

Description: a baseline model-free, offline, actor-critic method that forms the template for many of the other algorithms here.

Code: `deep_control.ddpg`

#### Twin Delayed DDPG (TD3)
Paper: [Addressing Function Approximation Error in Actor-Critic Methods](https://arxiv.org/abs/1802.09477), Fujimoto et al., 2018.

Description: Builds off of DDPG and makes several changes to improve the critic's learning and performance (Clipped Double Q Learning, Target Smoothing, Actor Delay).

Code: `deep_control.td3`

Other References: [author's implementation](https://github.com/sfujim/TD3)

#### Soft Actor Critic (SAC)
Paper: [Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor](https://arxiv.org/abs/1801.01290), Haarnoja et al., 2018.

Description: Samples actions from a stochastic actor rather than relying on added exploration noise during training. Uses a TD3-like double critic system. We *do* implement the learnable entropy coefficient approach described in the [follow-up paper](https://arxiv.org/abs/1812.05905).

Code: `deep_control.sac`

Other References: [Yarats and Kostrikov's implementation](https://github.com/denisyarats/pytorch_sac), [author's implementation](https://github.com/haarnoja/sac).

#### Pixel SAC with Data Augmentation (SAC+AUG)
Paper: [Measuring Visual Generalization in Continuous Control from Pixels](https://arxiv.org/abs/2010.06740)

Description: This is a pixel-specific version of SAC with a few tricks/hyperparemter settings to improve performance. We include many different data augmentation techniques, including those used in [RAD](https://arxiv.org/abs/2004.14990), [DrQ](https://arxiv.org/abs/2004.13649) and [Network Randomization](https://arxiv.org/abs/1910.05396). The DrQ augmentation is turned on by default, and has a huge impact on performance.

Code: `deep_control.sac_aug`

Other References: [SAC+AE code](https://github.com/denisyarats/pytorch_sac_ae), [RAD Procgen code](https://github.com/pokaxpoka/rad_procgen).

#### Self-Guided and Self-Regularized Actor-Critic (GRAC)
Paper: [GRAC: Self-Regularized Actor-Critic](https://arxiv.org/abs/2009.08973)

Description: GRAC is a combination of a stochastic policy with TD3-like stability improvements and CEM-based action selection like you'd see in Qt-Opt or CAQL.

Code: `deep_control.grac`

Othere References: [author's implementation](https://github.com/stanford-iprl-lab/GRAC)

#### Model Based Policy Optimization (MBPO)
Paper: [When to Trust Your Model: Model-Based Policy Optimization](https://arxiv.org/abs/1906.08253), Janner et al., 2019.

*Warning: in alpha*

Description: Improves SAC's sample efficiency by training the policy on transitions generated by a learned world model.

Code: `deep_control.mbpo`

Other References: [author's implementation](https://github.com/JannerM/mbpo).

### Installation
```bash
git clone https://github.com/jakegrigsby/deep_control.git
cd deep_control
pip install -e .
```

### Examples
see the `examples` folder for a look at how to train agents in environments like the DeepMind Control Suite and OpenAI Gym.

### Roadmap
Things that will hopefully be included by the end of 2020: 
1) [GRAC](https://arxiv.org/abs/2009.08973)
2) [CAQL](https://arxiv.org/abs/1909.12397)
3) Quick setup support for [Robosuite](https://robosuite.ai) and [CARLA](https://carla.org).
